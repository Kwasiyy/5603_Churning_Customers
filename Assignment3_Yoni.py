# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1U6xP9hwEJzYjF35R1ZmWVVWkTp3NqWep
"""

from google.colab import drive
drive.mount('/content/drive')

from google.colab import drive
import pandas as pd
import numpy as np
from sklearn import preprocessing
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import mutual_info_classif
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('/content/drive/MyDrive/CustomerChurn_dataset.csv')

df.head()

df = df.drop('customerID', axis = 1)

df.info()

df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')
df['TotalCharges'].fillna(method='ffill', inplace=True)

corr_matrix = df.corr(method= 'spearman')
plt.figure(figsize = (14, 7))
sns.heatmap(corr_matrix, annot = True, cmap='YlOrBr')

categorical_values = df.drop(['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges' ], axis = 1)

label_encoder = preprocessing.LabelEncoder()
categorical_df = categorical_values.apply(label_encoder.fit_transform)
categorical_df.head(14)

label_encoder = preprocessing.LabelEncoder()
categorical_df = categorical_values.apply(label_encoder.fit_transform)
new_numeric = df[['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges']]
df = pd.merge(new_numeric, categorical_df, left_index=True, right_index=True)
df

missing_values = df.isnull().sum()
missing_values

X = df.drop('Churn', axis=1)
y = df['Churn']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
rforrest_classifier = RandomForestClassifier(n_estimators=112, max_depth=12, criterion='entropy')

rforrest_classifier.fit(X_train, y_train)

feature_importance = rforrest_classifier.feature_importances_

for name, score in zip(X.columns, feature_importance):
  print(name,'=', score)

feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)
feature_importance_df.iloc[0:20]

X_data = df.drop('Churn', axis=1)
Y_data = df['Churn']
mi_scores = mutual_info_classif(X_data, Y_data)
mi_scores = pd.Series(mi_scores, index=X_data.columns)
mi_scores.sort_values(ascending=False, inplace=True)
print(mi_scores)

selected_features = ['TotalCharges',	'MonthlyCharges',	'tenure', 'Contract',	'OnlineSecurity',	'PaymentMethod' ,'TechSupport'	, 'InternetService'	,	'PaperlessBilling', 'DeviceProtection', 'Churn'  ]
df_selected = df[selected_features]

#training and test split
X = df_selected.drop('Churn', axis=1)
y = df_selected['Churn']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#Scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# MLP Model using Functional API
input_layer = Input(shape=(X_train_scaled.shape[1],))
hidden_layer_1 = Dense(64, activation='relu')(input_layer)
hidden_layer_2 = Dense(32, activation='relu')(hidden_layer_1)
output_layer = Dense(1, activation='sigmoid')(hidden_layer_2)

model = Model(inputs=input_layer, outputs=output_layer)
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.summary()


history = model.fit(X_train_scaled, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)

y_pred = model.predict(X_test_scaled)
y_pred_classes = (y_pred > 0.5).astype("int32")

accuracy = accuracy_score(y_test, y_pred_classes)
auc_score = roc_auc_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")
print(f"AUC Score: {auc_score:.2f}")
print(classification_report(y_test, y_pred_classes))

from sklearn.metrics import auc, roc_auc_score, roc_auc_score, roc_curve
from keras.callbacks import Callback

fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_test, y_pred)

auc_keras = auc(fpr_keras, tpr_keras)

plt.figure(1)
plt.plot([0,1], [0,1], 'k--')
plt.plot(fpr_keras, tpr_keras, label = 'Keras (area = {:.3f})'.format(auc_keras))
plt.xlabel('False postive rate')
plt.ylabel('True positive rate')
plt.title('ROC Curve')
plt.legend(loc='best')
plt.show()

param_grid = {
    'hidden_layer_sizes': [(10,), (50,), (100,)],
    'activation': ['relu', 'tanh', 'logistic'],
    'solver': ['sgd', 'adam'],
    'max_iter': [150, 200, 300],
    'alpha': [0.0001, 0.001, 0.01, 0.1],
    'learning_rate_init': [0.001, 0.01, 0.1],
    'batch_size': [32, 64, 128]
}

from sklearn.model_selection import KFold, GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.neural_network import MLPClassifier

!pip install keras-tuner

import keras_tuner
from tensorflow import keras

def build_model(hp):
    model = keras.Sequential()
    model.add(keras.layers.InputLayer(input_shape=(X_train.shape[1],)))

    for i in range(hp.Int('num_hidden_layers', min_value=1, max_value=4)):
        model.add(keras.layers.Dense(units=hp.Int(f'units_{i}', min_value=32, max_value=96, step=32), activation=hp.Choice(f'activation_{i}', values=['relu', 'tanh'])))

    model.add(keras.layers.Dense(1, activation='sigmoid'))

    learning_rate = hp.Float('lr', min_value=1e-4, max_value=1e-2, sampling='log')
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),
        loss='binary_crossentropy',
        metrics=['accuracy']
    )
    return model

tuner = keras_tuner.Hyperband(build_model, objective='val_accuracy', max_epochs=100, factor=3, directory='tuning_dir', project_name='samples')
tuner.search(X_train_scaled, y_train, epochs=10, validation_split=0.2)

best_model = tuner.get_best_models(num_models=2)[0]
best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]

best_model.summary()

y_pred = best_model.predict(X_test)
y_pred_binary = (y_pred > 0.5).astype(int)

best_model.save('optimal_model.h5')

import pickle
with open('Scaled.pkl', 'wb') as file:
  pickle.dump(scaler, file)